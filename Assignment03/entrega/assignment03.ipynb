{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Code source adapted from: Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "from cmath import sqrt\n",
    "from tqdm import tqdm\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_add_scatter(x, y, c='black'):\n",
    "    plt.scatter(x, y, color= c)\n",
    "\n",
    "def graph_add_line(x, y, c='black'):\n",
    "    plt.plot(x, y, color=c, linewidth=3)\n",
    "\n",
    "def plot(name=\"\"):\n",
    "    plt.xticks()\n",
    "    plt.yticks()\n",
    "    \n",
    "    if name!=\"\":\n",
    "        plt.savefig(name)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def distance(v1, v2):\n",
    "    dist = 0\n",
    "    for i in range(len(v1)):\n",
    "        dist += (v1[i]-v2[i])*(v1[i]-v2[i])\n",
    "    return sqrt(dist).real\n",
    "\n",
    "def GetClosest(v, i):\n",
    "    closest = -1\n",
    "    for j in range(len(v)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        if closest == -1 or distance(v[closest],v[i]) > distance(v[j],v[i]):\n",
    "            closest = j\n",
    "    return closest\n",
    "\n",
    "def GetGroup(labels, group, offset=0):\n",
    "    indexes = []\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == group:\n",
    "            indexes.append(i+offset)\n",
    "    return indexes\n",
    "\n",
    "class DataReader():\n",
    "    def __init__(self, file_path=\"health-dataset/health.txt\"):\n",
    "        f = open(file_path)\n",
    "        self.lines = []\n",
    "        for i, line in enumerate(f):\n",
    "            s = line.split(\"|\")\n",
    "            self.lines.append([str(i) + \" \" + s[-1], False])\n",
    "    \n",
    "    def GetLine(self, line):\n",
    "        if line < 0 and line >= len(self.lines):\n",
    "            return str(line) + \" Is Not a Valid Line\"\n",
    "        return self.lines[line][0]\n",
    "    \n",
    "    def GetLineGroup(self, lines):\n",
    "        t = []\n",
    "        for i in lines:\n",
    "            if not self.lines[i][1]:\n",
    "                t.append(self.lines[i][0])\n",
    "        return t\n",
    "    def DeleteLines(self, d):\n",
    "        for i in range(len(d)-1,-1,-1):\n",
    "            print(self.lines[i])\n",
    "            del self.lines[i]\n",
    "    def DeleteOutliers(self):\n",
    "        for i in range(len(self.lines)-1,-1,-1):\n",
    "            if self.lines[i][1]:\n",
    "                print(self.lines[i])\n",
    "                del self.lines[i]\n",
    "    def SetOutliers(self, o):\n",
    "        for i in o:\n",
    "            self.lines[i][1] = True\n",
    "\n",
    "data_reader = DataReader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and treat training dataset\n",
    "dataset = pandas.read_csv('health-dataset/word2vec.csv').values\n",
    "# dataset = (dataset-dataset.max()/2) / dataset.max()\n",
    "dataset = normalize(dataset)\n",
    "data_train = dataset[0:10000,:]\n",
    "data_validation = dataset[10000:13227,:]\n",
    "\n",
    "# dataset = load_digits(n_class=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_costs = []\n",
    "kmeans_clusters = []\n",
    "kmeans_silhouette = []\n",
    "kmeans_davies = []\n",
    "for n in range(10,100,10):\n",
    "    print(\"Training KMeans for \" + str(n) + \" clusters\")\n",
    "    kmeans = KMeans(n_clusters=n)\n",
    "    kmeans.fit(data_train)\n",
    "    \n",
    "    \n",
    "    labels = kmeans.predict(data_validation)\n",
    "    s = silhouette_score(data_validation,labels)\n",
    "    d = davies_bouldin_score(data_validation,labels)\n",
    "    print(\"\\t\\t Cost: \" + str(kmeans.inertia_))\n",
    "    print(\"\\t\\t Silhouette Score: \" + str(s))\n",
    "    print(\"\\t\\t Davies Bouldin Score: \" + str(d))\n",
    "    \n",
    "    kmeans_costs.append(kmeans.inertia_)\n",
    "    kmeans_clusters.append(n)\n",
    "    kmeans_silhouette.append(s)\n",
    "    kmeans_davies.append(d)\n",
    "    \n",
    "for n in range(100,2001,100):\n",
    "    print(\"Training KMeans for \" + str(n) + \" clusters\")\n",
    "    kmeans = KMeans(n_clusters=n)\n",
    "    kmeans.fit(data_train)\n",
    "    \n",
    "    labels = kmeans.predict(data_validation)\n",
    "    s = silhouette_score(data_validation,labels)\n",
    "    d = davies_bouldin_score(data_validation,labels)\n",
    "    print(\"\\t\\t Cost: \" + str(kmeans.inertia_))\n",
    "    print(\"\\t\\t Silhouette Score: \" + str(s))\n",
    "    print(\"\\t\\t Davies Bouldin Score: \" + str(d))\n",
    "    \n",
    "    kmeans_costs.append(kmeans.inertia_)\n",
    "    kmeans_clusters.append(n)\n",
    "    kmeans_silhouette.append(s)\n",
    "    kmeans_davies.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_add_line(kmeans_clusters, kmeans_costs)\n",
    "graph_add_scatter(kmeans_clusters, kmeans_costs,c='blue')\n",
    "plot(\"cost_nclusters_10_2000\")\n",
    "\n",
    "graph_add_line(kmeans_clusters, kmeans_silhouette)\n",
    "graph_add_scatter(kmeans_clusters, kmeans_silhouette,c='blue')\n",
    "plot(\"silhouette_nclusters_10_2000\")\n",
    "\n",
    "graph_add_line(kmeans_clusters, kmeans_davies)\n",
    "graph_add_scatter(kmeans_clusters, kmeans_davies,c='blue')\n",
    "plot(\"davies_nclusters_10_2000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Clusters\n",
    "\n",
    "Here we get the tweets using kmeans with 100 and 2000 clusters to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_100 = KMeans(n_clusters=100)\n",
    "kmeans_100.fit(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans_100.predict(data_validation)\n",
    "s100 = silhouette_score(data_validation,labels)\n",
    "print(\"Cost: \" + str(kmeans_100.inertia_))\n",
    "print(\"Silhouette Score for 100 Clusters is: \" + str(s100))\n",
    "d100 = davies_bouldin_score(data_validation,labels)\n",
    "print(\"Davies Bouldin Score for 100 Clusters is: \" + str(d100))\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    print(\"\\n\\nGroup \" + str(i))\n",
    "    print(\"Closest Group: \" + str(GetClosest(kmeans_100.cluster_centers_,i)) + \"\\n\")\n",
    "    pp.pprint(data_reader.GetLineGroup(GetGroup(labels,i,offset=10000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_1000 = KMeans(n_clusters=1000)\n",
    "kmeans_1000.fit(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans_1000.predict(data_validation)\n",
    "s1000 = silhouette_score(data_validation,labels)\n",
    "print(\"Silhouette Score for 1000 Clusters is: \" + str(s1000))\n",
    "d1000 = davies_bouldin_score(data_validation,labels)\n",
    "print(\"Davies Bouldin Score for 1000 Clusters is: \" + str(d1000))\n",
    "\n",
    "print(GetClosest(kmeans_1000.cluster_centers_,58))\n",
    "for i in range(1000):\n",
    "    print(\"\\n\\nGroup \" + str(i))\n",
    "    print(\"Closest Group: \" + str(GetClosest(kmeans_1000.cluster_centers_,i)) + \"\\n\")\n",
    "    pp.pprint(data_reader.GetLineGroup(GetGroup(labels,i,offset=10000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "print(\"Training Affinity\")\n",
    "affinity = AffinityPropagation()\n",
    "affinity.fit(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = affinity.predict(data_validation)\n",
    "print(\"Silhouette Score: \" + str(silhouette_score(data_validation,labels)))\n",
    "print(\"Davies Bouldin Score: \" + str(davies_bouldin_score(data_validation,labels)))\n",
    "\n",
    "print(len(affinity.cluster_centers_indices_))\n",
    "for i in range(len(affinity.cluster_centers_indices_)):\n",
    "    print(\"\\nGroup \" + str(i))\n",
    "    print(\"Closest Group: \" + str(GetClosest(affinity.cluster_centers_,i)) + \"\\n\")\n",
    "    pp.pprint(data_reader.GetLineGroup(GetGroup(labels,i, offset=10000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Analysis\n",
    "\n",
    "Here we use the best algorithm and cluster number we got from previous experiments and run it again using different number of features, using PCA to reduce dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = data_train.shape[1]\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(dataset)\n",
    "variance = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# Get Dimensionality with variance of 0.95 \n",
    "fn = 0\n",
    "for i in range(len(variance)):\n",
    "    if variance[i] > 0.95:\n",
    "        fn = i\n",
    "        break\n",
    "\n",
    "print(\"Found feature subset of size \" + str(fn+1) + \" with variance of \" + str(variance[fn]))\n",
    "pca = PCA(n_components=n_features-fn)\n",
    "data_train_pca = pca.fit_transform(data_train)\n",
    "\n",
    "# Fit Kmeans algorithm for 0.95 variance\n",
    "print(\"Training KMeans with \" + str(fn+1) + \" features\")\n",
    "kmeans_pca = KMeans(n_clusters=1000)\n",
    "kmeans_pca.fit(data_train_pca)\n",
    "\n",
    "data_val_pca = pca.fit_transform(data_validation)\n",
    "\n",
    "labels = kmeans_pca.predict(data_val_pca)\n",
    "\n",
    "s = silhouette_score(data_val_pca,labels)\n",
    "d = davies_bouldin_score(data_val_pca,labels)\n",
    "\n",
    "print(\"\\t\\t Data Variance: \" + str(pca.explained_variance_ratio_[0]))\n",
    "print(\"\\t\\t Silhouette Score: \" + str(s))\n",
    "print(\"\\t\\t Davies Bouldin Score: \" + str(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    print(\"\\n\\nGroup \" + str(i) + \"\\n\")\n",
    "    pp.pprint(data_reader.GetLineGroup(GetGroup(labels,i,offset=10000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = data_train.shape[1]\n",
    "\n",
    "kmeans_costs = []\n",
    "kmeans_feature_number = []\n",
    "kmeans_silhouette = []\n",
    "kmeans_davies = []\n",
    "for fn in range(1,n_features,10):\n",
    "    pca = PCA(n_components=fn)\n",
    "    pca.fit(data_train)\n",
    "\n",
    "    # Fit Kmeans algorithm for 0.95 variance\n",
    "    print(\"Training KMeans for 1000 clusters with \" + str(fn) + \" features\")\n",
    "    data_train_pca = pca.transform(data_train)\n",
    "    kmeans_pca = KMeans(n_clusters=1000)\n",
    "    kmeans_pca.fit(data_train_pca)\n",
    "\n",
    "    data_validation_pca = pca.transform(data_validation)\n",
    "    labels = kmeans_pca.predict(data_validation_pca)\n",
    "    s = silhouette_score(data_validation_pca,labels)\n",
    "    d = davies_bouldin_score(data_validation_pca,labels)\n",
    "    \n",
    "    print(\"\\t\\t Data Variance: \" + str(pca.explained_variance_ratio_.cumsum()[-1]))\n",
    "    print(\"\\t\\t Cost: \" + str(kmeans_pca.inertia_))\n",
    "    print(\"\\t\\t Silhouette Score: \" + str(s))\n",
    "    print(\"\\t\\t Davies Bouldin Score: \" + str(d))\n",
    "    \n",
    "    kmeans_costs.append(kmeans_pca.inertia_)\n",
    "    kmeans_feature_number.append(fn)\n",
    "    kmeans_silhouette.append(s)\n",
    "    kmeans_davies.append(d)\n",
    "    \n",
    "graph_add_line(kmeans_feature_number, kmeans_costs)\n",
    "graph_add_scatter(kmeans_feature_number, kmeans_costs,c='blue')\n",
    "plot(\"cost_features_10_2000\")\n",
    "\n",
    "graph_add_line(kmeans_feature_number, kmeans_silhouette)\n",
    "graph_add_scatter(kmeans_feature_number, kmeans_silhouette,c='blue')\n",
    "plot(\"silhouette_features_10_2000\")\n",
    "\n",
    "graph_add_line(kmeans_feature_number, kmeans_davies)\n",
    "graph_add_scatter(kmeans_feature_number, kmeans_davies,c='blue')\n",
    "plot(\"davies_features_10_2000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN - Detecting Outliers\n",
    "\n",
    "Since the experimentes were subpar, we try to detect outliers and cluster again using DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we try different values for the \"eps\" parameter, which is the maximum distance between two samples for them to be considered neighbours.\n",
    "\n",
    "We then plot graphs to see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eps = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "outliers = []\n",
    "clusters = []\n",
    "sil = []\n",
    "biggest_cluster = []\n",
    "for i in n_eps:\n",
    "    print(\"DBSCAN for eps = \" + str(i))\n",
    "    db = DBSCAN(eps=i)\n",
    "    db.fit(dataset)\n",
    "    labels = db.labels_\n",
    "    \n",
    "    sil.append(silhouette_score(dataset,labels))\n",
    "    \n",
    "    outliers.append(len(data_reader.GetLineGroup(GetGroup(labels,-1))))\n",
    "    \n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    clusters.append(n_clusters)\n",
    "    \n",
    "    biggest_cluster.append(max([len(data_reader.GetLineGroup(GetGroup(labels,i))) for i in range(n_clusters)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------------------------------------\")\n",
    "print(\"eps x number of clusters found\")\n",
    "graph_add_line(n_eps, clusters)\n",
    "graph_add_scatter(n_eps, clusters,c='blue')\n",
    "plot()\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "print(\"eps x silhouette score\")\n",
    "graph_add_line(n_eps, sil)\n",
    "graph_add_scatter(n_eps, sil,c='blue')\n",
    "plot()\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "print(\"eps x number of outliers\")\n",
    "graph_add_line(n_eps, outliers)\n",
    "graph_add_scatter(n_eps, outliers,c='blue')\n",
    "plot()\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "print(\"eps x biggest cluster\")\n",
    "graph_add_line(n_eps, biggest_cluster)\n",
    "graph_add_scatter(n_eps, biggest_cluster,c='blue')\n",
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the results above, we can see that between eps = 0.5 and eps = 0.8 the number of outliers diminishes and the size of the biggest cluster increases. We think that all the outliers are being grouped together. We can check that below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get outliers\n",
    "db = DBSCAN(eps=0.5)\n",
    "db.fit(dataset)\n",
    "\n",
    "labels = db.labels_\n",
    "\n",
    "outliers = data_reader.GetLineGroup(GetGroup(labels,-1))\n",
    "\n",
    "# Get Biggest cluster\n",
    "db = DBSCAN(eps=0.8)\n",
    "db.fit(dataset)\n",
    "\n",
    "labels = db.labels_\n",
    "\n",
    "group_lengths = [len(data_reader.GetLineGroup(GetGroup(labels,i))) for i in range(n_clusters)]\n",
    "biggest_c = data_reader.GetLineGroup(GetGroup(labels,group_lengths.index(max(group_lengths))))\n",
    "\n",
    "# Compare if biggest cluster has outliers\n",
    "common = 0\n",
    "for d in biggest_c:\n",
    "    if d in outliers:\n",
    "        common+=1\n",
    "        \n",
    "print(\"number of outliers = \" + str(len(outliers)))\n",
    "print(\"Number of elements in common = \" + str(common))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this result, we can see that most of the elements from the outliers are inside a new group. This proves that there are a lot of outliers in out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.5)\n",
    "db.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we remove all the outliers and fit DBSCAN again for the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = db.labels_\n",
    "\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"OUTLIERS\\n\")\n",
    "outliers = data_reader.GetLineGroup(GetGroup(labels,-1))\n",
    "print(\"There are \" + str(len(outliers)) + \" outliers in the dataset\\n\")\n",
    "pp.pprint(outliers)\n",
    "\n",
    "data_reader.SetOutliers([i for i,k in enumerate(labels) if k==-1])\n",
    "\n",
    "# Delete Outliers\n",
    "new_data = np.delete(dataset, [i for i,k in enumerate(labels) if k==-1], axis=0)\n",
    "data_reader.DeleteOutliers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN()\n",
    "db.fit(new_data)\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"SCORES\\n\")\n",
    "labels = db.labels_\n",
    "s = silhouette_score(new_data,labels)\n",
    "# print(\"Cost: \" + str(db.inertia_))\n",
    "print(\"Silhouette Score for 100 Clusters is: \" + str(s))\n",
    "d = davies_bouldin_score(new_data,labels)\n",
    "print(\"Davies Bouldin Score for 100 Clusters is: \" + str(d))\n",
    "\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(\"There are \" + str(n_clusters_) + \" different clusters\")\n",
    "\n",
    "print(len(db.components_))\n",
    "for i in range(n_clusters_):\n",
    "    print(\"\\n\\nGroup \" + str(i))\n",
    "    print(\"Closest Group: \" + str(GetClosest(db.components_,i)) + \"\\n\")\n",
    "    pp.pprint(data_reader.GetLineGroup(GetGroup(labels,i,offset=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
