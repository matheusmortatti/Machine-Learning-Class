{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Code source adapted from: Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treat_dataset(dataset):\n",
    "    vcut = {'Fair': 0, 'Good': 1, 'Very Good': 2, 'Premium': 3, 'Ideal': 4}\n",
    "    vcolor = {'D': 6, 'E': 5, 'F': 4, 'G': 3, 'H': 2, 'I': 1, 'J': 0}\n",
    "    vclarity = {'I1': 0, 'SI2': 1, 'SI1': 2, 'VS2': 3, 'VS1': 4, 'VVS2': 5, 'VVS1': 6, 'IF': 7}\n",
    "    \n",
    "    for row in dataset:\n",
    "        # Modify string to number values\n",
    "        row[\"cut\"] = vcut[row[\"cut\"]]\n",
    "        row[\"color\"] = vcolor[row[\"color\"]]\n",
    "        row[\"clarity\"] = vclarity[row[\"clarity\"]]\n",
    "        \n",
    "        # Normalize values\n",
    "        row[\"carat\"] = (float(row[\"carat\"]) - (0.2+5.01)/2)/(0.2+5.01)\n",
    "        row[\"cut\"] = (float(row[\"cut\"]) - (4/2))/4\n",
    "        row[\"color\"] = (float(row[\"color\"]) - (6/2))/6\n",
    "        row[\"clarity\"] = (float(row[\"clarity\"]) - (7/2))/7\n",
    "        row[\"x\"] = (float(row[\"x\"]) - (10.74/2))/10.74\n",
    "        row[\"y\"] = (float(row[\"y\"]) - (58.9/2))/58.9\n",
    "        row[\"z\"] = (float(row[\"z\"]) - (31.8/2))/31.8\n",
    "        row[\"depth\"] = (float(row[\"depth\"]) - (43+79)/2)/(43+79)\n",
    "        row[\"table\"] = (float(row[\"table\"]) - (43+95)/2)/(43+95)\n",
    "        \n",
    "        # Add X0 for ease of use\n",
    "        row[\"x0\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          carat   clarity     color   cut     depth  price     table  \\\n",
      "0     -0.189060 -0.071429 -0.333333  0.50  0.009016  10501 -0.101449   \n",
      "1     -0.442418 -0.214286  0.000000  0.25 -0.018033    574 -0.065217   \n",
      "2     -0.237044  0.071429  0.166667  0.50 -0.011475  11649 -0.086957   \n",
      "3     -0.436660  0.214286 -0.166667 -0.50  0.033607    922 -0.072464   \n",
      "4     -0.438580 -0.071429  0.166667  0.00 -0.018033    602 -0.050725   \n",
      "5     -0.402111 -0.357143  0.333333  0.25  0.013934   1205 -0.123188   \n",
      "6     -0.212092 -0.071429 -0.166667 -0.25  0.021311  10291 -0.079710   \n",
      "7     -0.336852  0.071429  0.000000  0.50  0.006557   4373 -0.101449   \n",
      "8     -0.436660  0.071429  0.166667  0.50  0.001639    723 -0.094203   \n",
      "9     -0.288868  0.071429  0.000000  0.50  0.002459   6535 -0.108696   \n",
      "10    -0.402111  0.214286  0.333333  0.00 -0.008197   2365 -0.079710   \n",
      "11    -0.396353 -0.071429  0.333333  0.50  0.005738   1754 -0.101449   \n",
      "12    -0.210173 -0.071429  0.000000  0.25  0.013934  12756 -0.079710   \n",
      "13    -0.365643 -0.214286  0.000000  0.25  0.016393   2386 -0.094203   \n",
      "14    -0.357965  0.214286  0.000000  0.50 -0.002459   3831 -0.094203   \n",
      "15    -0.285029  0.071429 -0.166667  0.25  0.009016   5845 -0.079710   \n",
      "16    -0.112284 -0.357143 -0.333333  0.50  0.009836  10412 -0.086957   \n",
      "17    -0.327255 -0.214286  0.500000  0.25  0.013115   4304 -0.079710   \n",
      "18    -0.390595 -0.071429  0.500000  0.50  0.002459   2546 -0.094203   \n",
      "19    -0.440499 -0.214286 -0.333333  0.00  0.018852    523 -0.115942   \n",
      "20    -0.402111 -0.071429 -0.166667  0.50 -0.001639   1442 -0.086957   \n",
      "21    -0.442418 -0.214286 -0.333333  0.50  0.010656    506 -0.094203   \n",
      "22    -0.400192 -0.071429  0.166667  0.50  0.002459   1624 -0.086957   \n",
      "23    -0.308061 -0.357143 -0.166667 -0.25  0.023770   3878 -0.101449   \n",
      "24    -0.440499 -0.214286  0.000000  0.50  0.004918    627 -0.094203   \n",
      "25    -0.442418 -0.357143  0.166667 -0.25  0.018852    394 -0.101449   \n",
      "26    -0.162188 -0.071429 -0.166667  0.25  0.009836  13626 -0.065217   \n",
      "27    -0.430902 -0.071429 -0.166667  0.25  0.000000    729 -0.065217   \n",
      "28    -0.267754 -0.071429 -0.166667  0.25  0.004098   6710 -0.079710   \n",
      "29    -0.304223 -0.357143  0.333333  0.50  0.004918   4824 -0.101449   \n",
      "...         ...       ...       ...   ...       ...    ...       ...   \n",
      "45819 -0.392514  0.071429  0.000000  0.50 -0.002459   1855 -0.101449   \n",
      "45820 -0.396353  0.214286  0.000000  0.50  0.011475   1962 -0.113043   \n",
      "45821 -0.430902 -0.071429  0.166667  0.50 -0.000820    932 -0.086957   \n",
      "45822 -0.419386  0.357143  0.000000  0.50  0.004918   1235 -0.101449   \n",
      "45823 -0.432821  0.071429  0.000000  0.25  0.001639    906 -0.094203   \n",
      "45824 -0.363724 -0.357143 -0.500000  0.00 -0.002459   1805 -0.057971   \n",
      "45825 -0.302303 -0.071429  0.166667  0.50  0.010656   7392 -0.094203   \n",
      "45826 -0.440499 -0.214286  0.500000  0.50  0.008197    732 -0.101449   \n",
      "45827 -0.382917  0.071429  0.333333  0.00 -0.011475   2025 -0.043478   \n",
      "45828 -0.440499 -0.071429 -0.166667  0.50  0.000000    507 -0.086957   \n",
      "45829 -0.417466  0.357143  0.333333  0.50  0.002459   1433 -0.086957   \n",
      "45830 -0.440499 -0.214286 -0.333333  0.25  0.010656    408 -0.072464   \n",
      "45831 -0.283109 -0.500000  0.000000  0.25  0.001639   3746 -0.079710   \n",
      "45832 -0.344530 -0.357143  0.333333  0.00  0.011475   2894 -0.086957   \n",
      "45833 -0.398273 -0.214286  0.500000  0.00 -0.015574   1415 -0.072464   \n",
      "45834 -0.348369 -0.214286 -0.333333 -0.25  0.023770   2095 -0.101449   \n",
      "45835 -0.344530 -0.071429  0.166667  0.50  0.006557   3624 -0.101449   \n",
      "45836 -0.423225 -0.071429 -0.166667 -0.25 -0.013115    945 -0.036232   \n",
      "45837 -0.304223 -0.357143  0.000000  0.25  0.005738   3027 -0.079710   \n",
      "45838 -0.442418 -0.071429  0.333333  0.50  0.006557    844 -0.094203   \n",
      "45839 -0.208253  0.071429 -0.333333  0.50 -0.002459  10392 -0.065217   \n",
      "45840 -0.365643 -0.214286  0.333333  0.25 -0.019672   2657 -0.065217   \n",
      "45841 -0.442418  0.071429  0.333333  0.50  0.012295    778 -0.094203   \n",
      "45842 -0.438580  0.500000  0.000000  0.50  0.002459    918 -0.108696   \n",
      "45843 -0.229367  0.214286  0.333333 -0.25 -0.009016  14853 -0.057971   \n",
      "45844 -0.177543  0.071429 -0.500000  0.50  0.000820  10681 -0.086957   \n",
      "45845 -0.380998 -0.357143  0.166667  0.50  0.006557   1364 -0.101449   \n",
      "45846 -0.392514 -0.357143 -0.166667  0.00 -0.012295   1144 -0.086957   \n",
      "45847 -0.350288 -0.214286  0.500000  0.50  0.009836   3389 -0.101449   \n",
      "45848 -0.346449 -0.214286  0.333333  0.50  0.005738   3630 -0.094203   \n",
      "\n",
      "              x  x0         y         z  \n",
      "0      0.201117   1 -0.371307 -0.352516  \n",
      "1     -0.091248   1 -0.426146 -0.419182  \n",
      "2      0.177840   1 -0.377419 -0.364151  \n",
      "3     -0.090317   1 -0.426655 -0.410692  \n",
      "4     -0.087523   1 -0.423939 -0.417610  \n",
      "5     -0.020484   1 -0.413073 -0.398742  \n",
      "6      0.172253   1 -0.376570 -0.355031  \n",
      "7      0.067039   1 -0.396265 -0.381447  \n",
      "8     -0.084730   1 -0.423769 -0.413836  \n",
      "9      0.122905   1 -0.387097 -0.371384  \n",
      "10    -0.015829   1 -0.411205 -0.401572  \n",
      "11    -0.012104   1 -0.410526 -0.398113  \n",
      "12     0.170391   1 -0.375891 -0.356918  \n",
      "13     0.028864   1 -0.404075 -0.387736  \n",
      "14     0.049348   1 -0.400170 -0.387421  \n",
      "15     0.114525   1 -0.387267 -0.370755  \n",
      "16     0.250466   1 -0.364346 -0.343082  \n",
      "17     0.072626   1 -0.396265 -0.379245  \n",
      "18    -0.001862   1 -0.407980 -0.396226  \n",
      "19    -0.097765   1 -0.426995 -0.414151  \n",
      "20    -0.021415   1 -0.412394 -0.401572  \n",
      "21    -0.094972   1 -0.426655 -0.415094  \n",
      "22    -0.013966   1 -0.411885 -0.399686  \n",
      "23     0.081937   1 -0.392530 -0.373585  \n",
      "24    -0.094041   1 -0.425297 -0.415094  \n",
      "25    -0.100559   1 -0.426486 -0.414151  \n",
      "26     0.218808   1 -0.369610 -0.349371  \n",
      "27    -0.071695   1 -0.422750 -0.412264  \n",
      "28     0.135009   1 -0.384890 -0.368553  \n",
      "29     0.101490   1 -0.389304 -0.374214  \n",
      "...         ...  ..       ...       ...  \n",
      "45819 -0.003724   1 -0.408998 -0.397799  \n",
      "45820 -0.013966   1 -0.410866 -0.397484  \n",
      "45821 -0.070764   1 -0.422241 -0.411950  \n",
      "45822 -0.048417   1 -0.418166 -0.406289  \n",
      "45823 -0.075419   1 -0.423090 -0.412579  \n",
      "45824  0.035382   1 -0.401358 -0.389623  \n",
      "45825  0.098696   1 -0.390492 -0.373899  \n",
      "45826 -0.092179   1 -0.425976 -0.414780  \n",
      "45827  0.006518   1 -0.405433 -0.396855  \n",
      "45828 -0.092179   1 -0.425127 -0.415723  \n",
      "45829 -0.049348   1 -0.417148 -0.406289  \n",
      "45830 -0.100559   1 -0.426655 -0.415723  \n",
      "45831  0.130354   1 -0.386927 -0.370755  \n",
      "45832  0.050279   1 -0.398302 -0.383333  \n",
      "45833 -0.009311   1 -0.409508 -0.401572  \n",
      "45834  0.044693   1 -0.401528 -0.383019  \n",
      "45835  0.054935   1 -0.399491 -0.384591  \n",
      "45836 -0.051210   1 -0.419355 -0.410692  \n",
      "45837  0.101490   1 -0.391171 -0.375157  \n",
      "45838 -0.095903   1 -0.426995 -0.416038  \n",
      "45839  0.189013   1 -0.374024 -0.358491  \n",
      "45840  0.038175   1 -0.402377 -0.393711  \n",
      "45841 -0.099628   1 -0.427334 -0.415723  \n",
      "45842 -0.089385   1 -0.424109 -0.414465  \n",
      "45843  0.171322   1 -0.374873 -0.362579  \n",
      "45844  0.211359   1 -0.369270 -0.352516  \n",
      "45845  0.012104   1 -0.406452 -0.393082  \n",
      "45846  0.002793   1 -0.407810 -0.398742  \n",
      "45847  0.051210   1 -0.398981 -0.383962  \n",
      "45848  0.057728   1 -0.397453 -0.383333  \n",
      "\n",
      "[45849 rows x 11 columns]\n",
      "         carat   clarity     color   cut     depth  price     table         x  \\\n",
      "0    -0.394434  0.071429  0.333333  0.50  0.004098   2220 -0.094203 -0.010242   \n",
      "1    -0.267754 -0.071429 -0.166667 -0.50  0.028689   5041 -0.094203  0.123836   \n",
      "2    -0.261996 -0.214286  0.000000  0.25 -0.017213   6145 -0.079710  0.159218   \n",
      "3    -0.267754 -0.214286  0.000000 -0.50  0.028689   4637 -0.094203  0.122905   \n",
      "4    -0.354127 -0.214286 -0.333333 -0.25  0.027049   2326 -0.065217  0.029795   \n",
      "5    -0.402111 -0.071429  0.333333 -0.25  0.022951   1662 -0.101449 -0.027002   \n",
      "6    -0.265835  0.071429  0.000000  0.50  0.012295   8608 -0.086957  0.135009   \n",
      "7    -0.421305  0.214286 -0.333333 -0.25  0.023770    827 -0.101449 -0.061453   \n",
      "8    -0.210173 -0.071429  0.333333  0.00 -0.001639  14338 -0.065217  0.179702   \n",
      "9    -0.440499  0.357143  0.166667  0.25 -0.013115    766 -0.079710 -0.089385   \n",
      "10   -0.256238  0.071429  0.000000  0.50 -0.011475   9262 -0.086957  0.157356   \n",
      "11   -0.382917 -0.071429  0.000000  0.25  0.016393   1800 -0.079710  0.005587   \n",
      "12   -0.390595 -0.071429 -0.500000  0.00 -0.006557   1072 -0.050725 -0.003724   \n",
      "13   -0.265835  0.071429 -0.500000  0.50  0.004098   5649 -0.101449  0.138734   \n",
      "14   -0.277351 -0.357143  0.500000  0.50  0.009016   5898 -0.108696  0.132216   \n",
      "15   -0.114203 -0.214286 -0.500000  0.00  0.005738  14125 -0.072464  0.245810   \n",
      "16   -0.306142 -0.071429  0.000000  0.50  0.006557   6504 -0.086957  0.098696   \n",
      "17   -0.421305  0.214286 -0.166667  0.00  0.011475    873 -0.108696 -0.057728   \n",
      "18   -0.365643 -0.357143 -0.333333  0.50  0.010656   1874 -0.101449  0.027933   \n",
      "19   -0.442418  0.214286 -0.166667  0.50  0.012295    567 -0.108696 -0.099628   \n",
      "20   -0.302303 -0.357143  0.166667  0.50  0.013115   3461 -0.086957  0.099628   \n",
      "21   -0.423225  0.357143 -0.166667  0.25  0.006557   1088 -0.079710 -0.060521   \n",
      "22   -0.327255  0.071429  0.333333 -0.25  0.013934   5201 -0.079710  0.065177   \n",
      "23   -0.427063  0.357143  0.000000  0.50  0.007377    993 -0.094203 -0.065177   \n",
      "24   -0.440499 -0.071429 -0.166667  0.50  0.016393    628 -0.086957 -0.095903   \n",
      "25   -0.440499 -0.071429 -0.166667  0.50  0.016393    628 -0.086957 -0.095903   \n",
      "26   -0.112284 -0.071429  0.000000  0.50  0.008197  18700 -0.086957  0.254190   \n",
      "27   -0.436660  0.214286  0.000000  0.00  0.014754    752 -0.086957 -0.091248   \n",
      "28   -0.212092 -0.357143  0.333333  0.25  0.013934   8316 -0.072464  0.179702   \n",
      "29   -0.404031 -0.357143  0.333333 -0.50 -0.024590    851 -0.014493 -0.010242   \n",
      "...        ...       ...       ...   ...       ...    ...       ...       ...   \n",
      "8061 -0.448177  0.071429  0.333333  0.50  0.004918    470 -0.086957 -0.116387   \n",
      "8062 -0.396353 -0.357143  0.166667  0.50  0.008197   1098 -0.108696 -0.013035   \n",
      "8063 -0.304223 -0.071429  0.000000  0.50  0.011475   5346 -0.086957  0.102421   \n",
      "8064 -0.453935  0.214286  0.166667  0.00  0.009836    552 -0.079710 -0.130354   \n",
      "8065 -0.440499 -0.071429  0.333333  0.25 -0.005738    872 -0.079710 -0.088454   \n",
      "8066 -0.290787 -0.214286 -0.166667  0.50  0.009016   5376 -0.101449  0.114525   \n",
      "8067 -0.354127 -0.214286  0.166667  0.50  0.013934   2607 -0.086957  0.041899   \n",
      "8068 -0.440499  0.071429  0.000000 -0.25  0.022131    802 -0.094203 -0.099628   \n",
      "8069 -0.342610 -0.357143  0.166667  0.25 -0.010656   2867 -0.086957  0.071695   \n",
      "8070 -0.415547 -0.357143  0.000000  0.00 -0.003279   1016 -0.101449 -0.040037   \n",
      "8071 -0.444338  0.071429  0.333333  0.00  0.007377    555 -0.101449 -0.101490   \n",
      "8072 -0.419386 -0.071429  0.000000  0.50  0.009836    847 -0.108696 -0.053073   \n",
      "8073 -0.114203 -0.071429 -0.333333  0.50  0.046721  15729 -0.094203  0.221601   \n",
      "8074 -0.427063 -0.357143  0.333333  0.25 -0.009016    596 -0.094203 -0.054004   \n",
      "8075 -0.304223 -0.500000 -0.166667  0.25  0.012295   3141 -0.065217  0.094972   \n",
      "8076 -0.327255  0.071429 -0.166667  0.00  0.016393   4939 -0.101449  0.075419   \n",
      "8077 -0.212092  0.214286  0.000000  0.00 -0.000820  12798 -0.094203  0.183426   \n",
      "8078 -0.404031 -0.071429  0.500000  0.50  0.010656   1845 -0.101449 -0.024209   \n",
      "8079 -0.292706  0.071429  0.000000  0.50  0.008197   6779 -0.101449  0.116387   \n",
      "8080 -0.321497 -0.357143  0.166667 -0.25  0.019672   3896 -0.108696  0.076350   \n",
      "8081 -0.296545  0.500000  0.000000  0.00  0.001639   8415 -0.086957  0.107076   \n",
      "8082 -0.365643  0.357143 -0.500000  0.25 -0.000820   1844 -0.079710  0.034451   \n",
      "8083 -0.354127 -0.214286  0.166667  0.25  0.008197   2706 -0.065217  0.047486   \n",
      "8084 -0.442418 -0.214286  0.166667  0.00  0.020492    641 -0.072464 -0.107076   \n",
      "8085 -0.417466  0.357143 -0.333333  0.50  0.009836    968 -0.086957 -0.049348   \n",
      "8086 -0.308061  0.071429  0.000000 -0.50 -0.018852   5853 -0.014493  0.102421   \n",
      "8087 -0.361804 -0.214286  0.166667  0.50  0.001639   2737 -0.101449  0.038175   \n",
      "8088 -0.365643 -0.214286  0.000000  0.25  0.003279   2394 -0.065217  0.029795   \n",
      "8089 -0.417466 -0.071429  0.166667  0.00  0.002459    905 -0.072464 -0.052142   \n",
      "8090 -0.421305 -0.071429  0.000000  0.50 -0.000820    827 -0.086957 -0.054004   \n",
      "\n",
      "      x0         y         z  \n",
      "0      1 -0.410187 -0.398113  \n",
      "1      1 -0.386927 -0.364465  \n",
      "2      1 -0.381494 -0.369811  \n",
      "3      1 -0.387606 -0.365094  \n",
      "4      1 -0.404244 -0.385535  \n",
      "5      1 -0.414261 -0.398428  \n",
      "6      1 -0.385059 -0.366352  \n",
      "7      1 -0.419525 -0.405031  \n",
      "8      1 -0.375042 -0.359748  \n",
      "9      1 -0.424448 -0.417296  \n",
      "10     1 -0.379626 -0.367296  \n",
      "11     1 -0.408319 -0.392767  \n",
      "12     1 -0.408319 -0.398428  \n",
      "13     1 -0.383362 -0.367296  \n",
      "14     1 -0.385569 -0.367925  \n",
      "15     1 -0.362649 -0.343711  \n",
      "16     1 -0.391171 -0.375157  \n",
      "17     1 -0.419015 -0.406604  \n",
      "18     1 -0.402886 -0.388365  \n",
      "19     1 -0.426995 -0.415094  \n",
      "20     1 -0.391341 -0.373585  \n",
      "21     1 -0.420374 -0.408491  \n",
      "22     1 -0.396095 -0.379874  \n",
      "23     1 -0.420204 -0.408805  \n",
      "24     1 -0.426655 -0.414151  \n",
      "25     1 -0.426655 -0.414151  \n",
      "26     1 -0.363328 -0.342453  \n",
      "27     1 -0.424109 -0.412579  \n",
      "28     1 -0.377419 -0.356918  \n",
      "29     1 -0.412224 -0.405031  \n",
      "...   ..       ...       ...  \n",
      "8061   1 -0.429372 -0.419811  \n",
      "8062   1 -0.410357 -0.397484  \n",
      "8063   1 -0.392020 -0.374214  \n",
      "8064   1 -0.431919 -0.422013  \n",
      "8065   1 -0.425806 -0.416667  \n",
      "8066   1 -0.387267 -0.370755  \n",
      "8067   1 -0.400679 -0.384906  \n",
      "8068   1 -0.427504 -0.414151  \n",
      "8069   1 -0.396095 -0.384906  \n",
      "8070   1 -0.415789 -0.405660  \n",
      "8071   1 -0.426486 -0.416352  \n",
      "8072   1 -0.418166 -0.405975  \n",
      "8073   1 -0.370119 -0.338365  \n",
      "8074   1 -0.419864 -0.410377  \n",
      "8075   1 -0.391171 -0.374214  \n",
      "8076   1 -0.395925 -0.377987  \n",
      "8077   1 -0.374873 -0.359119  \n",
      "8078   1 -0.413922 -0.400314  \n",
      "8079   1 -0.388455 -0.371384  \n",
      "8080   1 -0.394058 -0.376101  \n",
      "8081   1 -0.388625 -0.374214  \n",
      "8082   1 -0.401698 -0.389623  \n",
      "8083   1 -0.402207 -0.386478  \n",
      "8084   1 -0.428862 -0.416038  \n",
      "8085   1 -0.418336 -0.405660  \n",
      "8086   1 -0.390492 -0.380818  \n",
      "8087   1 -0.401188 -0.388365  \n",
      "8088   1 -0.402547 -0.389623  \n",
      "8089   1 -0.417827 -0.406918  \n",
      "8090   1 -0.417827 -0.407862  \n",
      "\n",
      "[8091 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read and treat training dataset\n",
    "dataset_train = []\n",
    "reader = csv.DictReader(open('diamonds-train.csv', 'r'))\n",
    "for line in reader:\n",
    "     dataset_train.append(line)\n",
    "\n",
    "treat_dataset(dataset_train)\n",
    "train_df = pd.DataFrame(dataset_train)\n",
    "print(train_df)\n",
    "\n",
    "# Read and treat test dataset\n",
    "dataset_test = []\n",
    "reader = csv.DictReader(open('diamonds-test.csv', 'r'))\n",
    "for line in reader:\n",
    "     dataset_test.append(line)\n",
    "\n",
    "treat_dataset(dataset_test)\n",
    "test_df = pd.DataFrame(dataset_test)\n",
    "print(test_df)\n",
    "\n",
    "# Auxilary vector for name to number mapping\n",
    "ds_index = [\"x0\", \"carat\", \"clarity\", \"color\", \"cut\", \"depth\", \"table\", \"x\", \"y\", \"z\", \"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_function(thetas, data):\n",
    "    m = len(data)\n",
    "    s = 0\n",
    "    for row in range(data):\n",
    "        h = 0\n",
    "        for i in range(thetas):\n",
    "            h += thetas[i] * row[ds_index[i]]\n",
    "        s += (h - float(row[\"price\"]))*(h - float(row[\"price\"]))\n",
    "    return (1/(2*m)) * s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21387.540620268242, 56335.38207639048, 3500.3465747941714, 1780.0263511806272, 484.4891375708398, -9044.444555218413, -4386.116955667563, -8366.306011698367, 718.0776041618209, -5893.918139873633]\n"
     ]
    }
   ],
   "source": [
    "# STOCHASTIC WITH NUMBER OF ITERATIONS\n",
    "\n",
    "# Parameters\n",
    "# Alpha\n",
    "learningRate = 1e-02\n",
    "# Number of cases in training\n",
    "m = len(dataset_train)\n",
    "# Thetas Vector\n",
    "thetas = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "# Number of iterations\n",
    "iterations = 10000000\n",
    "\n",
    "# In the Stochastic mode, the error is calculated using only one row of the data set\n",
    "# We'll use them one by one \n",
    "row = 0\n",
    "while(iterations > 0):\n",
    "    \n",
    "    new_thetas = thetas.copy() \n",
    "        \n",
    "    # For each theta we do the following\n",
    "    for k in range(len(thetas)):\n",
    "        h = 0\n",
    "        \n",
    "        for i in range(len(thetas)):\n",
    "            h += thetas[i] * dataset_train[row][ds_index[i]]\n",
    "\n",
    "        # Updating the new thetas vector values\n",
    "        new_thetas[k] = thetas[k] - (learningRate * (h - float(dataset_train[row][\"price\"])) * dataset_train[row][ds_index[k]])\n",
    "        \n",
    "    # Updating row that will be used to calculate the error\n",
    "    row = (row + 1) % m\n",
    "            \n",
    "     # Atualization of the values of the thetas\n",
    "    thetas = new_thetas.copy()\n",
    "    iterations = iterations - 1\n",
    "\n",
    "# Printing Thetas     \n",
    "print(thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20335.04534561404, 56366.87506396955, 3560.2557854440156, 2103.5441517503214, 445.4072566808454, -8765.98482168368, -4373.871137887192, -7803.213670927394, -774.3454900983065, -6991.270432234771]\n"
     ]
    }
   ],
   "source": [
    "# STOCHASTIC WITH STOP CONDITION\n",
    "\n",
    "# Parameters\n",
    "# Alpha\n",
    "learningRate = 1e-02\n",
    "# Number of cases in training\n",
    "m = len(dataset_train)\n",
    "# Thetas Vector\n",
    "thetas = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "# Stop Conditions\n",
    "stopCondition = 1e-06\n",
    "done = False\n",
    "\n",
    "# In the Stochastic mode, the error is calculated using only one row of the data set\n",
    "# We'll use them one by one \n",
    "row = 0\n",
    "while(not done):\n",
    "    \n",
    "    new_thetas = thetas.copy() \n",
    "        \n",
    "    # For each theta we do the following\n",
    "    for k in range(len(thetas)):\n",
    "        h = 0\n",
    "        \n",
    "        for i in range(len(thetas)):\n",
    "            h += thetas[i] * dataset_train[row][ds_index[i]]\n",
    "\n",
    "        # Updating the new thetas vector values\n",
    "        new_thetas[k] = thetas[k] - (learningRate * (h - float(dataset_train[row][\"price\"])) * dataset_train[row][ds_index[k]])\n",
    "        \n",
    "    # Updating row that will be used to calculate the error\n",
    "    row = (row + 1) % m\n",
    "            \n",
    "    # If the change in value for new thetas is too small, we can stop iterating\n",
    "    done = True\n",
    "    for k in range(len(thetas)):\n",
    "        done = abs(thetas[k] - new_thetas[k]) < stopCondition and done       \n",
    "    \n",
    "\n",
    "    # Atualization of the values of the thetas\n",
    "    thetas = new_thetas.copy()\n",
    "\n",
    "# Printing Thetas\n",
    "print(thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6317.864335434728, 8806.014460402721, -223.1998447334793, -728.6385472819542, 305.79922514735097, 4.499858042021787, -301.74968886652266, 12053.276565945655, -384.8146964431467, -6.621754364416988]\n"
     ]
    }
   ],
   "source": [
    "# BATCH GRADIENT ALGORITHM WITH NUMBER OF ITERATIONS\n",
    "\n",
    "# Parameters\n",
    "# Alpha\n",
    "learningRate = 1e-01\n",
    "# Number of cases in training\n",
    "m = len(dataset_train)\n",
    "# Thetas Vector\n",
    "thetas = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "# Number of iterations\n",
    "iterations = 500\n",
    "\n",
    "\n",
    "while(iterations > 0):\n",
    "    \n",
    "    new_thetas = thetas.copy()\n",
    "    \n",
    "    # For each theta we do the following\n",
    "    for k in range(len(thetas)):\n",
    "        \n",
    "        s = 0\n",
    "        # We add every row of the dataset to the error calculation (Batch)\n",
    "        for row in dataset_train:\n",
    "            \n",
    "            h = 0\n",
    "            # Calculating the value for theta\n",
    "            for i in range(len(thetas)):\n",
    "                h += thetas[i] * row[ds_index[i]]\n",
    "                \n",
    "            s += (h - float(row[\"price\"])) * row[ds_index[k]]\n",
    "        \n",
    "        # Updating the new thetas vector values\n",
    "        new_thetas[k] = thetas[k] - ((learningRate / m) * s)\n",
    "\n",
    "    # Atualization of the values of the thetas\n",
    "    thetas = new_thetas.copy()\n",
    "    iterations = iterations - 1\n",
    "    \n",
    "# Printing Thetas    \n",
    "print(thetas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH GRADIENT ALGORITHM WITH STOP CONDITION\n",
    "\n",
    "# Parameters\n",
    "# Alpha\n",
    "learningRate = 1e-06\n",
    "# Number of cases in training\n",
    "m = len(dataset_train)\n",
    "# Thetas Vector\n",
    "thetas = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "# Stop Conditions\n",
    "iterations = 1e-03\n",
    "done = False\n",
    "\n",
    "while(not done):\n",
    "    \n",
    "    new_thetas = thetas.copy()\n",
    "    \n",
    "    # For each theta we do the following\n",
    "    for k in range(len(thetas)):\n",
    "        \n",
    "        s = 0\n",
    "        # We add every row of the dataset to the error calculation (Batch)\n",
    "        for row in dataset_train:\n",
    "            \n",
    "            h = 0\n",
    "            # Calculating the value for theta\n",
    "            for i in range(len(thetas)):\n",
    "                h += thetas[i] * row[ds_index[i]]\n",
    "                \n",
    "            s += (h - float(row[\"price\"])) * row[ds_index[k]]\n",
    "        \n",
    "        # Updating the new thetas vector values\n",
    "        new_thetas[k] = thetas[k] - ((learningRate / m) * s)\n",
    "    \n",
    "    # If the change in value for new thetas is too small, we can stop iterating\n",
    "    done = True\n",
    "    for k in range(len(thetas)):\n",
    "        done = abs(thetas[k] - new_thetas[k]) < stopCondition and done\n",
    "    \n",
    "    # Atualization of the values of the thetas\n",
    "    thetas = new_thetas.copy()\n",
    "    \n",
    "# Printing Thetas    \n",
    "print(thetas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.43604698697453, -2.9691764985381304, 0.3645707306582003, 2.693597835348101, 4.811201079709588, 0.9958056491450533, -0.01281832570631184, 1.4521309395756004, -3.5853871249860463, -3.4449224040921576]\n"
     ]
    }
   ],
   "source": [
    "# MINI BATCH GRADIENT ALGORITHM WITH NUMBER OF ITERATIONS\n",
    "\n",
    "# Parameters\n",
    "# Alpha\n",
    "learningRate = 1e-01\n",
    "# Number of cases in training\n",
    "m = len(dataset_train)\n",
    "# Thetas Vector\n",
    "thetas = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "# Number of iterations\n",
    "iterations = 100000\n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "while(iterations > 0):\n",
    "    \n",
    "    new_thetas = thetas.copy()\n",
    "    \n",
    "    # Step through the dataset in chuncks\n",
    "    for row in range(0, len(dataset_train), batch_size):\n",
    "        # For each theta we do the following\n",
    "        for k in range(len(thetas)):\n",
    "\n",
    "            s = 0\n",
    "            # We add every row of the dataset to the error calculation (Batch)\n",
    "            for offset in range(batch_size):\n",
    "                if row + offset >= m:\n",
    "                    break\n",
    "                \n",
    "                h = 0\n",
    "                # Calculating the value for theta\n",
    "                for i in range(len(thetas)):\n",
    "                    h += thetas[i] * dataset_train[row+offset][ds_index[i]]\n",
    "\n",
    "                s += (h - float(dataset_train[row][\"price\"])) * dataset_train[row+offset][ds_index[k]]\n",
    "\n",
    "            # Updating the new thetas vector values\n",
    "            new_thetas[k] = thetas[k] - ((learningRate / m) * s)\n",
    "        \n",
    "        iterations = iterations - 1\n",
    "\n",
    "    # Atualization of the values of the thetas\n",
    "    thetas = new_thetas.copy()\n",
    "    \n",
    "# Printing Thetas    \n",
    "print(thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6145\n",
      "164.70116871484413\n"
     ]
    }
   ],
   "source": [
    "print(dataset_test[2][\"price\"])\n",
    "\n",
    "teste = 0\n",
    "for i in range(len(thetas)):\n",
    "    teste += thetas[i] * dataset_test[2][ds_index[i]]\n",
    "\n",
    "print(teste)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
