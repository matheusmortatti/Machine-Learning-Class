\documentclass[conference]{IEEEtran}

\usepackage[utf8]{inputenc}

\PassOptionsToPackage{bookmarks=false}{hyperref}

\usepackage{amsmath}
\usepackage{float}

\usepackage{listings}
\usepackage{color}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage[caption=true,font=footnotesize]{subfig}
\usepackage[hidelinks]{hyperref}

\graphicspath{ gfx/ }

\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
		\node[shape=circle,draw,inner sep=1pt] (char) {#1};}}

\lstset{
  basicstyle=\normalsize ,
  backgroundcolor=\color{lbcolor},
  %frame=tb,
  language=C++,
  aboveskip=6mm,
  belowskip=3mm,
  numbers=left,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  captionpos=b,
  tabsize=3
}

% Additional new commands
\newcommand{\etal}{{\em et al. }}
\newcommand{\cL}{{\cal L}}
\newcommand{\map}{\texttt{map} }
\newcommand{\unmap}{\texttt{unmap} }
\newcommand{\rb}{\texttt{readBuffer} }
\newcommand{\rsec}[1]{Section~\ref{sec:#1}}
\newcommand{\rsecs}[2]{Sections~\ref{sec:#1} --~\ref{sec:#2}}
\newcommand{\rtab}[1]{Table~\ref{tab:#1}}
\newcommand{\rfig}[1]{Figure~\ref{fig:#1}}
\newcommand{\rfigs}[2]{Figures~\ref{fig:#1} --~\ref{fig:#2}}
\newcommand{\rlst}[1]{Listing~\ref{lst:#1}}
\newcommand{\rfign}[3]{Figures~\ref{fig:#1}, \ref{fig:#2} \& \ref{fig:#3}}
\newcommand{\rlsts}[2]{Listings~\ref{lst:#1}~--~\ref{lst:#2}}
\newcommand{\rlstn}[3]{Listings~\ref{lst:#1}{#2}~--~\ref{lst:#1}{#3}}
\newcommand{\req}[1]{Equation~\ref{eq:#1}}
\newcommand{\reqs}[2]{Equations~\ref{eq:#1} --~\ref{eq:#2}}
\newcommand{\ttt}[1]{{\texttt{#1}}}
\newcommand{\tit}[1]{{\textit{#1}}}
\newcommand{\citex}[1]{\cite{XX}\ }

\newenvironment{filecode}[1][]
{\minipage{0.9\linewidth}% \begin{filecode}[#1]
	\lstset{basicstyle=\ttfamily\footnotesize,frame=single,float=ht,#1}}
{\endminipage}% \end{filecode}

\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage[portuguese, english]{babel}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Estudos em Regressão Linear}

\author{\IEEEauthorblockN{Matheus Mortatti Diamantino}
\IEEEauthorblockA{
RA 156740 \\
matheusmortatti@gmail.com}
\and
\IEEEauthorblockN{Jos\'{e} Renato Vicente}
\IEEEauthorblockA{
RA 155984\\
joserenatovi@gmail.com}}

\maketitle

\section{Introdução}

Este projeto teve como intuito o estudo prático do método de regressão linear em Machine Learning. Foram utilizado os algoritmos de \textit{Gradient Descent} conhecidos como \textit{Stochastic, Batch e Mini Batch} bem como a \textit{Equação Normal}, que resolve o problema da Regressão Linear por uma equação matricial fechada, de modo a compara-los em termos de complexidade e acurácia.~\cite{sandra}

Foi feito um estudo de predição de preço de diamantes, utilizando uma base de dados com 54000 exemplos, em que são apresentados seus preços e nove features como tamanho, cor e número de quilates.

\section{Atividades}

\subsection{Regressão Linear~\cite{handson}}

Regressão Linear é um método muito conhecido de Machine Learning, utilizado para predizer o valor de uma variável dependente baseado em valores de variáveis independentes. Essa regressão é chamada linear porque se considera que a relação da resposta às variáveis é uma função linear de alguns parâmetros. Desta forma, dado um vetor Theta de tamanho igual ao número de features, cujo valor queremos determinar, temos que:

\begin{equation} \label{eq:hx}
PreçoAlvoEsperado = \sum_{i=1}^{m} \theta_{i}X_{i} = h_{\theta}(x)
\end{equation}

Em que X é um vetor com os valores das features para um dado diamante, cujo preço queremos determinar. Para encontrar esse valor de Theta, utilizaremos alguns algoritmos e compararemos os resultados obtidos com cada um.

Cada algoritmo utilizado é baseado no método de \textit{Descrida de Gradiente (ou Gradient Descent)}. Este é um método utilizado para achar o ponto mínimo de uma função, aproximando gradativamente seu valor até um ponto quando não é possível ser diminuido mais (i.e. a derivada da função neste ponto é zero). Este método consegue apenas achar mínimos locais e, com isso, não é garantido que o resultado obtido é o melhor para o dado problema.

Para medirmos a eficácia do algoritmo, utilizamos uma \textit{Função de Custo} que nos diz o quão perto do resultado desejado estamos, dado um conjunto de dados. Esta função é definida por:

\begin{equation} \label{eq:cost_function}
J(\theta) = \dfrac{1}{2m} \sum_{i=1}^{m}(h_{\theta}(x^{i}) - y^{i})^2
\end{equation}

Como queremos minimizar a função de custo, queremos que cada passo da nossa descida de gradiente se aproxime mais do mínimo local. Para extrairmos a direção que temos que ir, utilizamos a derivada da função de custo:

\begin{equation} \label{eq:cost_derivative}
\dfrac{\partial J}{\partial \theta_{j}} = \dfrac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{i}) - y^{i}) x^{i}
\end{equation}

Logo, para aproximarmos os valores de $\theta$ de modo a nos aproximar do mínimo local, utilizamos a fórmula

\begin{equation} \label{eq:gradient_descent}
\theta_{j} := \theta_{j} - \alpha \dfrac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{i}) - y^{i}) x^{i}
\end{equation}

, onde $0 \leq j \leq m$, $x_{0} = 1$ e $\alpha$ é o que chamamos de \textit{Learning Rate} que define o quão agressivamente tentaremos nos aproximar do mínimo. Este método de descida de gradiente é chamada de \textit{Batch Gradient Descent}. A seguir, veremos duas outras variações deste algoritmo e uma forma não iterativa para o problema.

\paragraph{Stochastic Gradient Descent}

Neste método, utiliza-se apenas um exemplo de treino para cada passo da descida de gradiente. Deste modo, a equação~\ref{eq:gradient_descent} se transforma em

\begin{equation} \label{eq:stochastic_descent}
\theta_{j} := \theta_{j} - \alpha(h_{\theta}(x^{i}) - y^{i}) x^{i}
\end{equation}

Utiliza-se este método quando procura-se rapidez de execução. Contudo, um ponto negativo deste método é que, como usamos como amostra apenas um exemplo de treino, um passo pode nos levar a um custo mais alto. Como o número de iterações é alta, porém, o método converge ao mínimo e mais eficientemente do que o \textit{Batch} até um certo limite.

\paragraph{Mini Batch Gradient Descent}

Para obtermos um resultado balanceado, utiliza-se uma mistura dos métodos \textit{Batch} e \textit{Stochastic}, em que define-se um tamanho para o lote de exemplos de treino que serão utilizados para atualizar cara $\theta$.

\begin{equation} \label{eq:minibatch_descent}
\theta_{j} := \theta_{j} - \alpha \dfrac{1}{m} \sum_{i=k}^{k+b-1}(h_{\theta}(x^{i}) - y^{i}) x^{i}
\end{equation}

, onde $b$ é o tamanho do lote, $k = 0, b, 2b, ..., m-1$.

\paragraph{Equação Normal}

Para a regressão linear, é possível derivar uma fórmula direta para o ponto de mínimo local que desejamos. Para isso, utilizamos manipulações matriciais na forma~\cite{ayearofai}

\begin{equation} \label{eq:normal}
\theta = (X^{T}X)^{-1}X^{T}y
\end{equation}

, onde $X$ é a matriz de features, $y$ é a matriz dos dados que queremos prever e $\theta$ é a matriz dos coeficientes de $h(X)$.

\subsection{Normalização de Features}

Como cada feature tem seu valor em uma escala diferente (i.e. algumas estão na ordem de milhares e outras na ordem de centenas), o processo de descida do gradiente poderá acontecer de forma lenta. Isso se dá pelo fato de que a atualização dos $\theta$s não ocorrerá de forma uniforme entre as features, já que a distância de um dado $\theta_{i}$ a seu valor esperado pode ser maior do que de outro $\theta_{j}, j \neq i$. Assim, realizamos o que é chamado de \textit{Normalização de Features}, onde colocamos todas as features $x_{i}$ em um valor entre $ \-0.5 \leq x_{i} \leq 0.5$. Isso é feito através da fórmula:

\begin{equation} \label{eq:feature_norm}
x_{i} = \dfrac{x_{i} - \dfrac{size(x_{i})}{2}}{size(x_{i})}
\end{equation}

\subsection{Transformação de Features Com Valores Não Reais}

Em alguns casos, podemos ter features que não possuem um valor no domínio dos números reais. Por exemplo, uma feature pode representar a cor de um dado elemento. Deste modo, é necessário realizar uma transformação de tais features para o domínio dos reais. Para realizar tal transformação, criamos uma nova feature para cada possível valor da feature que queremos transformar, de modo que, se para o exemplo de dado $e_{i}$ temos que esta feature possui um valor $x_{j}$, a nova feature correspondente a $x_{j}$ terá o valor $1$ e as demais features criadas terão o valor $0$. Tomemos como exemplo uma feature de Cor que pode receber os valores \textit{Azul, Amarelo, Vermelho e Verde}. Assim, o resultado da transformação acontecerá da seguinte forma:

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
Feature $x_{i}$ & Azul & Amarelo & Vermelho & Verde \\ \hline
Azul            & 1    & 0       & 0        & 0     \\ \hline
Amarelo         & 0    & 1       & 0        & 0     \\ \hline
Vermelho        & 0    & 0       & 1        & 0     \\ \hline
Verde           & 0    & 0       & 0        & 1     \\ \hline
\end{tabular}
\end{table}

Onde cada coluna representa a nova feature criada e cada linha representa o valor original da feature $x_{i}$. Chamamos este método de \textit{Grid}.

\subsection{Regularização}

Regularização é um método utilizado para \textit{evitar} overfitting dos dados. É relizado de forma a penalizar os valores de $\theta$ para que estes mantenham valores pequenos, sendo menos propenso ao overfitting. Isto é feito na forma de um novo parâmetro $\lambda$ que definirá o quanto cada $\theta$ será penalizado:

\begin{equation} \label{eq:regularization}
\theta_{j} := \theta_{j} * (1 - \lambda * \dfrac{\alpha}{m}) - \alpha \dfrac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{i}) - y^{i}) x^{i}
\end{equation}

Este valor deve ser modificado de forma a melhorar o resultado final. Se o valor for muito alto, a influência da regularização pode ser negativa, criando um aumento no erro final. Se muito baixo, não terá o efeito desejado. Vale mencionar que a regularização na equação normal se da pela forma

\begin{equation} \label{eq:normal_reg}
\theta = (X^{T}X + \lambda I)^{-1}X^{T}y
\end{equation}

, onde $I$ é a matriz identidade.

\subsection{Base de Dados}

Foi utilizado uma base de dados sobre Diamantes, a qual descreve diferentes aspectos sobre o diamante como cor, claridade e quilate, e apresenta seu preço. O objetivo deste projeto foi de utilizado o método de \textit{Regressão Linear} descrito acima para prever o preço de um dado diamante, dado suas características.

\section{Soluções Propostas}

\subsection{Tratamento dos Dados}

Para obtermos melhores resultados, começamos por realizar o tratamento dos dados. Para isso, foram aplicados os métodos de \textit{Normalização} descritos na seção anterior. Também, foi aplicado o método de \textit{Grid} nas features \textit{Cut, Color e Clarity}, pois seus valores são descritos por uma string.

\subsection{Algoritmos Implementados}

Com o objetivo de compara-los, foram implementados os algoritmos \textit{Batch, Mini Batch, Stochastic e Equação Normal} como solução para a Regressão Linear.

\paragraph{Algoritmos Iterativos}

Algoritmos iterativos para a \textit{Descida de Gradiente} precisam de um critério de parada para sua execução. Tais critérios foram, demonstrados pela Listagem~\ref{lst:criterio_parada}:

\begin{itemize}
  \item Número máximo de iterações.
  \item Valor máximo da a diferença entre os $\theta$s originais e novos.
\end{itemize}

\begin{filecode}[label=lst:criterio_parada,caption=Criterios de Parada para os algoritmos de Descida do Gradiente]
  \lstset{numbers=left}
	\lstinputlisting[language=Python]{gfx/criterio_parada.py}
\end{filecode}

Foi implementado também uma solução para o caso onde a função de custo diverge devido a um $\alpha$ muito alto. Caso o custo corrente seja maior do que o último custo calculado, o valor de $\alpha$ é diminuido por um fator pré definido, de modo que a descida aconteça com menores riscos de divergência, como demonstrado pela Listagem~\ref{lst:learning_rate_change}.

\begin{filecode}[label=lst:learning_rate_change,caption=Modo como o fator $\alpha$ é modificado]
  \lstset{numbers=left}
	\lstinputlisting[language=Python]{gfx/learning_rate_change.py}
\end{filecode}

\paragraph{Regularização}

Para todas as soluções implementadas, foi utilizado o método de regularização para melhorar os resultados e evitar \textit{Overfitting}. Foram seguidos os métodos descritos acima para a implementação tanto na forma da \textit{Equação Normal} quanto para os algoritmos iterativos.

\section{Experimentos e Discussão}

Os experimentos foram realizados em uma máquina que possui um processador Intel Core i7-6700HQ com 4 cores rodando a 2.60GHz e 16GB de RAM, com Ubuntu 16.04.

\subsection{Comparação de Tratamento de Dados}

\paragraph{Aplicação de Normalização}

Os resultados obtidos rodando Stochastic Gradient Descent para dados sem normalização foram de $812.20$ para o erro absoluto nos dados de treino e $743406.95$ para o custo.

\begin{figure}[H]
  \includegraphics[width=\linewidth]{gfx/S_costitr_mitr2000000_nonnormalized.png}
  \caption{Custo x Iterações Para Stochastic Sem Normalização}
  \label{fig:not_normalized}
\end{figure}

Em acordo com os resultados obtidos, era esperado que os resultados fossem melhores para os dados normalizados, que se encontram na Tabela \ref{table:results_train}. A diferença foi pequena, pois os valores encontrados nas diversas features nao são muito diferentes (mesma ordem para todos os dados).

A Figura~\ref{fig:not_normalized} nos mostra que o caminho para a minimização do custo sem normalização teve muito mais ruido, sendo necessário o ajuste do \textit{Learning Rate} quando o valor divergia. Isso se dá pois atualizamos os valores dos coeficientes para cada exemplo de treino e, como os valores não estão normalizados, isso ocorre de forma não uniforme, fazendo com que o custo varie muito mais.

\paragraph{Aplicação de Grid}

Os resultados obtidos utilizando a Equação Normal para dados sem \textit{Grid} foram de $794.47$ para o erro absoluto nos dados de treino. Com a aplicação de \textit{Grid}, os resultados foram de $729.33$ para o erro absoluto.

Os resultados melhores para a aplicação de grid mostram que a aplicação de uma ordem de valores para as features não reais não funciona muito bem, pois isto infere que os valores para tais features possuem uma relação de grandeza crescente ou decrescente, o que pode não ser o caso.

\subsection{Aplicação da Regularização}

Os resultados obtidos utilizando a Equação Normal para dados com e sem a \textit{Regularização} foram feitos sem a aplicação de \textit{Grid}, com \textit{Normalização}.

Tais resultados foram de $794.47$ com $\lambda = -1.5$ e de $805.15$ com $\lambda = 0$ nos dados de treino. A pequena diferença ocorre pelo fato de que a regressão linear, por utilizar apenas termos de ordem primária em sua função $h(\theta)$, não sofre de um \textit{Overfitting} significativo, não necessitando da aplicação da regularização para evitá-lo.

Com a aplicação de todos os métodos de tratamento de dados descritos até aqui, foram testados valores para $\lambda$ de modo a acharmos seu valor ótimo. Isso pode ser ilustrado pela Figura~\ref{fig:reg}.

\begin{figure}[H]
  \includegraphics[width=\linewidth]{gfx/normaleq_lambdaxcost_1.png}
  \caption{Erro X Lambda na Equação Normal}
  \label{fig:reg}
\end{figure}

\subsection{Comparação entre Algoritmos}

Os algorítmos de Gradient Descent mostraram ter uma diferença de performance muito grande. Para um tempo de execução consideravelmente menor, Stochastic obteve resultados muito melhores do que as duas outras variantes, que demoraram muito mais para convergir, como podemos ver pela Tabela~\ref{table:results_train}. Por observar apenas um diamante do dataset de treino a cada atualização de Theta, a atualização deste vetor é muito mais rápida. Desta forma, ele chega mais próximo do mínimo local da função do que Batch e Mini Batch.

Porém, caso o algorítmo fosse executado por tempo suficiente, as outras variantes obteriam resultados com erros menores, já que, por utilizar mais do que um diamante do dataset de treino para atualizar Theta, suas atualizações são mais precisas e, no caso do Batch, tem certeza que o novo vetor de thetas leva a um resultado mais proximo do minimo local, o que não ocorre com o Stochastic.

O método da equação normal foi o de melhor desempenho, tendo tempo de execução muito inferior aos demais, além de ser o método com os melhores resultados. Isso se deve pois, para datasets com até dezenas de milhares de entradas e centenas de features, a equação normal é rápida do que os métodos por convergência, o que deixa de ser o caso a medida que o dataset e o número de features aumenta.

\begin{table}[H]
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|c|}
\hline
               & Absolute Error & Final Cost    & Run Time(s) & Iterations \\ \hline
Stochastic     & 753.24      & 645820.77     & 676         & 2.000.000  \\ \hline
Mini Batch     & 991.72      & 1002188.59    & 1693.64     & 2.000      \\ \hline
Batch          & 2554.09     & 6426353.60    & 4142.068    & 1000       \\ \hline
Equação Normal & 729.33      & 561271.499184 & 0.455       & ------     \\ \hline
Scikit Learn   & 739.09      & -----         & 3561.999    & 200.000    \\ \hline
\end{tabular}
}
\caption{Tabela de Resultados de Treino Para Cada Algoritmo}
\label{table:results_train}
\end{table}

\vspace{-2em}

\begin{table}[H]
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
               & Absolute Error & $\lambda$ & $\alpha$ \\ \hline
Stochastic     & 743.73         & 0.0000005 & 0.01     \\ \hline
Equação Normal & 726.32         & -1.5      & -----    \\ \hline
Scikit-Learn   & 731.10         & 0.0000005 & 0.01     \\ \hline
\end{tabular}%
}
	\caption{Tabela de Resultados de Validação Para Os Algoritmos Mais Eficientes}
\label{table:results_test}
\end{table}

Comparando os valores obtidos com a implementação para o algoritmo \textit{Stochastic} feita pelos autores deste projeto e a feita pela framework \textit{Scikit-Learn}~\cite{scikit}, vemos que eles são muito similares em termos do erro final obtido, tanto para os dados de treino (Tabela~\ref{table:results_train}) quanto para os dados de validação (Tabela~\ref{table:results_test}). Contudo, vemos divergências no número de iterações e no tempo de execução. Enquando o scikit obteve resultados muito próximos da Equação Normal em apenas \textit{200.000} iterações, nossa implementação precisou de $2x$ mais. Em contrapartida, scikit teve um tempo de execução aproximadamente $5x$ maior, o que pode ser explicado pelos recursos que o algoritmo utiliza para evitar overfitting, divergência, entre outros detalhes de implementação. Vale notar também que a base de dados utilizada é relativamente simples e não necessita de um algoritmo com a complexidade do encontrado na framework, explicando os bons resultados obtidos pela nossa implementação.

\begin{figure}[H]
  \includegraphics[width=\linewidth]{gfx/S_costitr_mitr2000000.png}
  \caption{Custo x Iterações Para Stochastic}
  \label{fig:costs_sto}
\end{figure}

Observando a Figura~\ref{fig:costs_sto}, podemos notar a rapidez com que o algoritmo \textit{Stochastic} converge no início de sua operação. Contudo, após aproximadamente $250.000$ iterações, a taxa de convergência diminui significativamente. A figura também nos mostra como um \textit{Learning Rate} bem definido afeta  o resultado final. Se compararmos com a Figura~\ref{fig:not_normalized}, podemos ver que esta possui um ruído muito significativo, proveniente, dentre outros motivos, de uma taxa de aprendizado muito agressiva para a função.

\subsection{Regressão Polinomial}

Foram feitos breves experimentos com \textit{Regressão Polinomial} para verificar seus impactos nos resultados. Para isso, foi utilizado o método da Equação Normal e as features foram modificadas de modo a incluir termos polinomiais. Como podemos ver pela Tabela~\ref{table:results_poli}, os resultados para a regressão polinomial não são significativamente melhores que os da regressão linear. Isso pode ser explicado pelo \textit{Erro Irredutível} inerente dos dados utilizados. Isto significa que a qualidade dos dados utilizados não é boa o suficiente para realizar um treino e uma predição precisa. Vale lembrar também que o quanto mais aumentamos o coeficiente polinomial das features, maiores as chances de \textit{Overfitting}, mesmo que minimizados pelo processo de Regularização.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|}
\hline
               								& Absolute Error (Validation Data) \\ \hline
$X$     											& 726.32                        \\ \hline
$X + X^{2}$ 									& 720.94                        \\ \hline
$X + X^{2} + X^{3}$   				& 705.90                        \\ \hline
$X + X^{2} + X^{3} + X^{4}$   & 705.34                        \\ \hline
\end{tabular}
\caption{Tabela de Resultados da Regressão Polinomial}
\label{table:results_poli}
\end{table}

\section{Conclusões}


Após extensiva análise dos resultados, é seguro dizer que os estudos realizados em \textit{Regressão Linear} foram profundos o suficiente para obter um entendimento maior sobre o assunto.
Pode-se ver os efeitos de que um bom tratamento de dados, como normalização e aplicação de grids, pode trazer no resultado final, com melhora de 7.2\% com o uso do primeiro e 8.9\% com o segundo.
Analisando as diferenças de performance e resultados de cada método, pode-se entender quando cada abordagem é mais adequada. Os métodos que melhor desempenharam nesse projeto foram a equação normal e Stochastic Gradient Descent.
Os resultados foram em linha com o esperado mesmo com erros relativamente altos. Para obter erros menores, novas abordagens de solução do problema podem ser utilizadas, seja um outro tipo de regressão, como a polinomial que obeteve resultados um pouco melhores, seja com técnicas mais poderosas.


\bibliographystyle{IEEEtran}
\bibliography{biblio-link,biblio}

\end{document}
